{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN3050/4050 - Week 10: Deep Learning Glimpse\n",
    "\n",
    "## Deep Learning\n",
    "Deep learning is a big topic, and we can't hope to cover it in any depth here. But, as an introduction we are going to take a look at some state of the art tools used in both research and industry.\n",
    "\n",
    "We are also going to take a look at one of the biggest problems with deep models, the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports used in the rest of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1) Introduction to TensorFlow 2.0\n",
    "TensorFlow is a machine learning framework designed specifically for neural networks, but can be used for all sorts of computations. TensorFlow have highly optimized and accelerated code for most of the operations needed to make state of the art neural networks.\n",
    "\n",
    "In this course we will only be using TensorFlow but there exists many other deep learning frameworks, e.g. [PyTorch](https://pytorch.org/), [MXNet](https://mxnet.apache.org/), [Caffe](https://caffe.berkeleyvision.org/), and many more.\n",
    "\n",
    "This [tf tutorial](https://www.tensorflow.org/tutorials/customization/basics) is a useful reference while completing this exercise.\n",
    "\n",
    "**Note:** It's important to use TensorFlow 2.0 or later for these exercises as they make frequent use of eager execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple operations/tensors\n",
    "\n",
    "#### Tensors\n",
    "In TensorFlow the basic object is a tensor. Tensors work much like numpy arrays, but have a different/stricter syntax. A tensor can be made from a numpy array, and can be cast to a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(np.arange(1, 10).reshape((3, 3)))\n",
    "print(\"Original tensor:\")\n",
    "print(x)\n",
    "\n",
    "print(\"\\nConverted to array:\")\n",
    "print(x.numpy())\n",
    "print(type(x.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations\n",
    "TensorFlow also comes with all of the standard math operations. Again, these look very similar to the ones found in numpy. At this point you might wonder why use tensors when they are so similar to numpy arrays. Later in this assignment we are going to use some of the cool features of TensorFlow tensors that numpy arrays don't have.\n",
    "\n",
    "#### tf.constant vs. tf.Variable\n",
    "In TensorFlow we have two different kind of values. Constants, that are supposed to stay constant, and Variables that may change during calculations.\n",
    "\n",
    "**Constants:**\n",
    "- Initialized with a specific value.\n",
    "\n",
    "e.g `x = tf.constant(3, dtype=int)`\n",
    "- Not meant to change during runtime.\n",
    "\n",
    "**Variables:**\n",
    "- Initialized with a value OR an operation/function. e.g:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init = tf.random_uniform_initializer(minval=-1, maxval=1, seed=100) # Initializer from the TensorFlow package.\n",
    "\n",
    "x = tf.Variable(initial_value=x_init(shape=(1, 2), dtype='float32')) # A tf.variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- May change during runtime by using the `.assign()` or `assign_add()` methods.\n",
    "\n",
    "There are other differences we will see later.\n",
    "\n",
    "#### Exercise 1.1)\n",
    "Use TensorFlow tensors to do the following:\n",
    "\n",
    "1. Create two constants x, and y with the values, 3 and 7.\n",
    "2. Multiply the values and assign the result to `z`.\n",
    "3. Create a variable matrix `A` with the shape $3 \\times 3$ and a column vector `b` with 3 elements using the `x_init()` initializer.\n",
    "4. Multiply the matrix and the vector together and assign the result to `c`.\n",
    "5. Create a numpy column vector of shape (3, 1) with the values [1, 2, 3] and add these elementwise to `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "x = tf.constant(3)\n",
    "y = tf.constant(7)\n",
    "\n",
    "# 2)\n",
    "z = x*y\n",
    "\n",
    "# 3)\n",
    "A = tf.Variable(x_init(shape=(3, 3)))\n",
    "b = tf.Variable(x_init(shape=(3, 1)))\n",
    "\n",
    "# 4)\n",
    "c = tf.matmul(A, b)\n",
    "\n",
    "# 5) Update b with new values:\n",
    "d = np.array([[1], [2], [3]])\n",
    "\n",
    "b.assign_add(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2) Activation function\n",
    "Implement the sigmoid activation function and its derivative using TensorFlow. The exponential function is available as `tf.exp(x)`.\n",
    "\n",
    "Activation:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Gradient:\n",
    "$$\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Return sigmoid activation of x.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tf.Tensor\n",
    "        Tensor to calculate activations for.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    a : tf.Tensor\n",
    "        The activations.\n",
    "    \"\"\"\n",
    "    a = 1./(1 + tf.exp(-x))\n",
    "    return a\n",
    "\n",
    "def my_sigmoid_grad(a):\n",
    "    \"\"\"\n",
    "    Returns the gradient of the sigmoid function from the value of the activation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a : tf.Tensor\n",
    "        Output from my_sigmoid().\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grad : tf.Tensor\n",
    "        Gradient of the sigmoid function.\n",
    "    \"\"\"\n",
    "    grad = a*(1 - a)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can of course plot the values in TensorFlow tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(np.linspace(-10, 10, 100))\n",
    "a = my_sigmoid(x)\n",
    "grad = my_sigmoid_grad(a)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].plot(x, a); ax[0].set_title(\"Activation\")\n",
    "ax[1].plot(x, grad); ax[1].set_title(\"Gradient\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Tape\n",
    "TensorFlow comes with its own automatic differentiation module called gradient tape. Using gradient tape allows us to get the gradient of a series of TensorFlow operations without doing any manual differentiation. TensorFlow is not the only framework that has this feature, e.g. PyTorch's Autograd.\n",
    "\n",
    "#### Exercise 1.3)\n",
    "Evaluate your function `my_sigmoid()` in all the points of the tensor `x` in the cell below, and use `tf.GradientTape()` to calculate the gradient of the function in these points. You can look at [this](https://www.tensorflow.org/tutorials/customization/autodiff) page for help.\n",
    "\n",
    "Compare to your own implementation of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(np.linspace(-10, 10, 100))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    a = my_sigmoid(x)\n",
    "\n",
    "# Gradient from GradientTape():\n",
    "grad_tape = t.gradient(a, x)\n",
    "\n",
    "# Gradient from your own function:\n",
    "grad = my_sigmoid_grad(a)\n",
    "\n",
    "\n",
    "# Plot of the gradient from tf.GradientTape() and my_sigmoid_grad().\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x, grad_tape, label=\"Using Gradient Tape\")\n",
    "ax.plot(x, grad, '.', label=\"my_sigmoid_grad\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did everything correctly the dots should be on top of the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "TensorFlow comes with many implementations of functions useful to machine learning. The Keras module, `tf.keras`, is a higher lever API that allows you to very quickly and easily create complex models without needing to worry about all the minor details.\n",
    "\n",
    "#### Keras Sequential Model\n",
    "There are very few limits on what you can do with the Keras module, but to keep it simple we will restrict our models to \"sequential\" models. This means that the layers are all stacked on top each other and there are no loops, skip connections or forks. One layer feeds in to the next. These types of models are very easy to define using the Keras API.\n",
    "\n",
    "Below we define a simple network with an input layer of size 4,  two hidden layers of size 2 and a output layer of only one node. All the layers use a simple sigmoid activation function except the last one, where we skip the activation. Notice that we specify an initializer for our weights.\n",
    "\n",
    "![Model structure](figures/4_nn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "\n",
    "# The input_shape argument is a tuple with the last element empty.\n",
    "# This tells tensorflow that it can recieve any number of entries in this dimension.\n",
    "# That way we can train over batches of varying sizes later.\n",
    "model.add(tf.keras.layers.Dense(2, input_shape=(4,),\n",
    "                                activation=tf.keras.activations.sigmoid,\n",
    "                                kernel_initializer=x_init)) # We add the x_init initializer for our weights.\n",
    "\n",
    "# No need to define input sizes to the next layers. These sizes are infered from the previous layers.\n",
    "model.add(tf.keras.layers.Dense(2, activation=tf.keras.activations.sigmoid,\n",
    "                                kernel_initializer=x_init))\n",
    "model.add(tf.keras.layers.Dense(1, kernel_initializer=x_init))\n",
    "\n",
    "model.summary()\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.4)\n",
    "Use the `tf.keras` module to define a function that return a sequential model. The model should have input size of 2, with any number of examples in a batch, three hidden layers with two nodes and one output layer with two nodes. All layers except the last layer should apply the activation function. Also use the initializer specified in the function.\n",
    "\n",
    "![Model structure](figures/2_nn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(activation):\n",
    "    \"\"\"\n",
    "    Return a sequential model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : function\n",
    "        Function from tf.keras.activations, or my_sigmoid from earlier in the exercise.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : tf.keras.Sequential\n",
    "        Keras model.\n",
    "    \"\"\"\n",
    "    init = tf.random_uniform_initializer(minval=-1, maxval=1, seed=100) # Use this initialiser for all the layers.\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(2, input_shape=(2,), activation=activation, kernel_initializer=init))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(2, activation=activation, kernel_initializer=init))\n",
    "    model.add(tf.keras.layers.Dense(2, activation=activation, kernel_initializer=init))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(2, kernel_initializer=init))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2) Vanishing Gradients\n",
    "In this section we are going to look at a two different ways to cope with the vanishing gradient problem.\n",
    "\n",
    "- Change of activation function\n",
    "- \"Proper\" initialization of weights\n",
    "\n",
    "There are other techniques that can have a big impact on this problem, but these are the simplest and easiest to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "The sigmoid activation function you implemented above\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "has two properties that make it good for neural networks.\n",
    "- It's differentiable everywhere.\n",
    "- Its gradient has a very simple form based on the output of the function.\n",
    "\n",
    "$$\\frac{d}{dx}\\sigma(x) = \\sigma(x)(1 - \\sigma(x))$$\n",
    "\n",
    "However, there are some issues with this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1)\n",
    "Can you name and explain two problems with this activation function?\n",
    "\n",
    "**hint:** what happens when you evaluate the function for high or low values of $x$? And what is the domain of the function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "#### Saturation of the neurons\n",
    "\n",
    "For activations \"far\" from zero, the function flattens out, or saturates. The gradient saturates to zero in both directions, meaning that a high activation leads to a tiny gradient. This in turn means we may need many many epochs of training to make progress towards our goal.\n",
    "\n",
    "\n",
    "#### Not zero centered\n",
    "\n",
    "The input to the activation function of a neuron is\n",
    "\n",
    "$$\n",
    "h = \\sum_{\\forall i} w_ix_i + b\n",
    "$$\n",
    "The derivative of this sum for a specific $w_i$ is\n",
    "\n",
    "$$\n",
    "\\frac{dh}{dw_i} = x_i\n",
    "$$\n",
    "\n",
    "When we want to update our weights we use the chain rule and get something like\n",
    "\n",
    "$$\n",
    "\\frac{dL}{dw_i} = \\frac{dL}{dh}\\frac{dh}{dw_i} = \\frac{dL}{dh}x_i\n",
    "$$\n",
    "\n",
    "With the sigmoid activation function the $x_i$ that comes from the previous layer will always be positive. This means that the sign of the derivative, or the direction of change in $w_i$ is only determined by $dL/dh$. But our loss function is a scalar! this means that all the different $w_i$'s is going to share sign, either all positive, or all negative. This means that there are directions in the optimization space we can't go when optimizing through backpropagation.\n",
    "\n",
    "e.g for two weights, we can get gradient vectors in the first and third quadrant only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2)\n",
    "Use the function `build_model()` to create a model using the sigmoid activation function.\n",
    "\n",
    "In the next code block we run one iteration over a training set and make a \"violin\" plot of the gradients. What do you see? give a short description of the plot. Is the results suprising?\n",
    "\n",
    "You can use your own implementation of the sigmoid from earlier, or you can use TensorFlow's own sigmoid implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = build_model(tf.keras.activations.sigmoid)\n",
    "model_sig = build_model(my_sigmoid)\n",
    "\n",
    "from week10 import plot_grad\n",
    "fig = plot_grad(model_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The size of the gradient goes towards zero as we move back through the layers. Should not be suprising as we are using a naive initialization and a saturating activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better activation functions, tanh\n",
    "#### Exercise 2.3) Can we solve the problems by using the tanh activation function? why? why not?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The tanh function is zero centered giving outputs that can be negative. This opens up directions in our optimization space not available if we use the sigmoid function.\n",
    "\n",
    "The tanh function still saturates at the extremes, so we still expect to see the gradients go towards zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.4)\n",
    "Create a model using the tanh activation function and plot the gradients again. Comment on the result, and how it differs from the sigmoid case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tanh = build_model(tf.keras.activations.tanh)\n",
    "\n",
    "fig = plot_grad(model_tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The gradient propagates much better backwards in the layers. Still the gradient tends toward zero, but the problem is less severe compared to the sigmoid(logistic) activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better activation functions, ReLU\n",
    "The last activation function we are going to take a look at is the Rectified Linear Unit. This function is very different from the sigmoid and the tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "a = tf.keras.activations.relu(x)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, a)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is defined as\n",
    "\n",
    "$$    f(x)= \n",
    "\\begin{cases}\n",
    "    0, & \\text{if } x < 0\\\\\n",
    "    x, & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "#### Exercise 2.5)\n",
    "Can this activation function solve some of our problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The gradient of this activation function does not saturate. The gradient is either zero for values below zero, or one for values above.\n",
    "\n",
    "However, this function is not zero centered. so we do get issues with directions in the optimization space being unavailable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.6)\n",
    "As before, build a new model with this activation function and comment on the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_relu = build_model(tf.keras.activations.relu)\n",
    "                    \n",
    "fig = plot_grad(model_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The gradient does not vanish, but are clustered close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dead neurons\n",
    "The ReLU activation function has the problem that the sigmoid and tanh functions do not have. During training we can end up in a situation where the input to the activation function in a node is always less than zero. In this case the gradient going back through that node will ALWAYS be zero, i.e the node \"dies\" and does no longer take part in training.\n",
    "\n",
    "Other activation functions have been proposed to deal with this, from the simple Leaky-ReLU to the more interesting SELU. We will not explore these here, but when you are training your own models they are worth taking a look at."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing weights\n",
    "Before we start training a neural network we must initialize the weights and biases of the network. There are several ways we can do this.\n",
    "\n",
    "#### Exercise 2.7) Random uniform initialization\n",
    "Imagine you are going to train a network with *many* nodes in the hidden layers using $\\tanh$ as an activation function. You decide to initialize the weights of the network using a random uniform distribution in the range [-1, 1]. Can you think of any issues with this approach?\n",
    "\n",
    "**hint:** What happens to the sum inside the activation function as the number of neurons in the previous layer increase?\n",
    "\n",
    "**Answer:**\n",
    "The activation of a neuron is on the form (ignoring bias)\n",
    "\n",
    "$$\\tanh\\left(\\sum_{\\forall i} x_{i}w_{i}\\right)$$\n",
    "\n",
    "Variance of sum assuming independent variables($x_{i}$):\n",
    "\n",
    "$$\\text{Var}\\left(\\sum_{\\forall i} x_{i}w_{i}\\right) = \\sum_{\\forall i} \\text{Var}\\big(x_{i}w_{i}\\big)$$\n",
    "\n",
    "For the gradient of this activation to to not vanish, the term inside the $\\tanh$ must be \"close\" to zero. But as the number of neurons increase the variance of the sum also increase, meaning that it's likely that we get an activation in the saturated region of the function.\n",
    "\n",
    "One way to avoid this is to make the initialization depend on the number of neurons/weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glorot normal initialization\n",
    "Glorot initialization is a scheme where the size of the intitial weights depends on the number of neurons/weights in a layer. In this variant the weights are initialized as samples from a normal distribution.\n",
    "\n",
    "$$W_l \\sim \\mathcal{N}(\\mu=0, \\sigma_l)$$\n",
    "\n",
    "And $\\sigma_l$ is on the form\n",
    "\n",
    "$$\\sigma_l = \\sqrt{\\frac{2}{n_l + n_{l+1}}}$$\n",
    "\n",
    "Where $n_i$ is the number of neurons in layer $i$. The weight matrix $W_l$ is of size $n_l\\times n_{l+1}$.\n",
    "\n",
    "As the number of neurons and weights **increase**, the \"range\" of the initial values **decrease** so that the activations are likely to stay in the center range of the activation function where the gradient is large. The expression for $\\sigma$ has a theoretical underpinning that we won't look at here, but if interested you can check out the [paper by Xavier Glorot and Yoshua Bengio](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi). In it, they use a uniform distribution as an example. But the results are general for any kind of (reasonable) distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.8)\n",
    "Define a new function `build_model_glorot()`, that uses a glorot normal initialization in the layers of the model. You can copy and adapt the function you implemented in exercise 1.4\n",
    "\n",
    "The docs for the keras Glorot normal initializer can be found [here](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal?version=stable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_glorot(activation):\n",
    "    \"\"\"\n",
    "    Return a sequential model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    activation : function\n",
    "        Function from tf.keras.activations, or my_sigmoid from earlier in the exercises.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : tf-keras.Sequential\n",
    "        Keras model.\n",
    "    \"\"\"\n",
    "    init = tf.keras.initializers.GlorotNormal(seed=100) # Use this initialiser for all the layers.\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(2, input_shape=(2,), activation=activation,\n",
    "                                    kernel_initializer=init))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(2, activation=activation,\n",
    "                                    kernel_initializer=init))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(2, activation=activation,\n",
    "                                    kernel_initializer=init))\n",
    "    \n",
    "    model.add(tf.keras.layers.Dense(2, kernel_initializer=init))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients with better initialization\n",
    "\n",
    "#### Sigmoid\n",
    "Let's take a look at the gradients when we are using a sigmoid activation function and the Glorot normal intiialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_sig = build_model_glorot(tf.keras.activations.sigmoid)\n",
    "                    \n",
    "fig = plot_grad(model2_sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the figure from exercise 2.2, Notice the scale on the y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "We can see a marked improvement in the backpropagation of the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tanh\n",
    "Now compare the tanh activation function model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_tanh = build_model_glorot(tf.keras.activations.tanh)\n",
    "                    \n",
    "fig = plot_grad(model2_tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The scale of the activations has increased, And the graident does not seem to vanish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU\n",
    "Finally, compare a new ReLU model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2_relu = build_model_glorot(tf.keras.activations.relu)\n",
    "                    \n",
    "fig = plot_grad(model2_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The scale of the gradients have increased a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Frameworks\n",
    "In this exercise we have taken a look at the TensorFlow framework. A deep learning framework takes a lot of the pain out of creating complex models, and allows for quick development of performant code. In the third mandatory we will continue our exploration of tensorflow.\n",
    "\n",
    "### Vanishing Gradient\n",
    "Two ways to reduce the problem of vanishing gradients are to use a suitable activation function and a initialization scheme.\n",
    "\n",
    "The sigmoid(logistic) function has a few problems that we can improve upon with other activation functions, and a proper initialization of the weights can make a huge difference.\n",
    "\n",
    "We have only scratched the surface of possible activation functions and initializers. And there are also many other techniques that are employed to speed up training of neural networks, but these are left for later courses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
